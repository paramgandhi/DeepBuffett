{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "STAT 453: Deep Learning (Spring 2020)    \n",
    "Instructor: Sebastian Raschka (sraschka@wisc.edu)  \n",
    "\n",
    "Course website: http://pages.stat.wisc.edu/~sraschka/teaching/stat453-ss2020/   \n",
    "GitHub repository: https://github.com/rasbt/stat453-deep-learning-ss20\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ritikkumar Goyal\n",
    "ragoyal2@wisc.edu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import random\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No need to change anything here!\n",
    "# If there is a GPU available, it will use it,\n",
    "# otherwise, it will use the CPU\n",
    "\n",
    "RANDOM_SEED = 123\n",
    "DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset consists of 10 classes similar to the original MNIST dataset. Also, it shares the same overall structure with MNIST, i.e., there are 60k training images and 10k test images, and all images are black & white images of size 28x28. \n",
    "\n",
    "Below is an example of how the images look like:\n",
    "\n",
    "![](figures/fashion-mnist-sprite.png)\n",
    "\n",
    "(Image Source: https://github.com/zalandoresearch/fashion-mnist)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 10 classes in this dataset are\n",
    "\n",
    "\n",
    "| Label | Description |\n",
    "| --- | --- |\n",
    "| 0 | T-shirt/top |\n",
    "| 1 | Trouser |\n",
    "| 2 | Pullover |\n",
    "| 3 | Dress |\n",
    "| 4 | Coat |\n",
    "| 5 | Sandal |\n",
    "| 6 | Shirt |\n",
    "| 7 | Sneaker |\n",
    "| 8 | Bag |\n",
    "| 9 | Ankle boot |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you continue, please execute the companion notebook \"Notebook for Preparing the Dataset for HW3\" ([`hw3-dataprep.ipynb`](hw3-dataprep.ipynb)) for downloading and preparing the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "\n",
    "\n",
    "# class FashionMNISTDataset(Dataset):\n",
    "#     \"\"\"Custom Dataset for loading FashionMNIST images\"\"\"\n",
    "\n",
    "#     def __init__(self, csv_path):\n",
    "#         df = pd.read_csv(csv_path)\n",
    "#         self.img_dir = img_dir\n",
    "#         self.img_names = df['image_name'].values\n",
    "#         self.y = df['class_label'].values\n",
    "#         self.transform = transform\n",
    "\n",
    "#     def __getitem__(self, index):\n",
    "#         img = Image.open(os.path.join(self.img_dir,\n",
    "#                                       self.img_names[index]))\n",
    "        \n",
    "#         if self.transform is not None:\n",
    "#             img = self.transform(img)\n",
    "        \n",
    "#         label = self.y[index]\n",
    "#         return img, label\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return self.y.shape[0]\n",
    "    \n",
    "class StockDataset(Dataset):\n",
    "    def __init__(self, csv_path):\n",
    "        df = pd.read_csv(csv_path)[0:1000]\n",
    "#         self.y = 100*((df['Target'].to_numpy().astype('float32') / df['Stock Price'].to_numpy().astype('float32')) - 1)\n",
    "        self.y = df['Label'].to_numpy().astype('float32')\n",
    "        self.features = df.drop([\"Company\", \"Date\", \"Target\", \"Label\"], axis=1).to_numpy().astype('float32')\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        features = self.features[index]\n",
    "        label = self.y[index]\n",
    "        \n",
    "        return features, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.y.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################\n",
    "# THIS CELL CAN BE MODIFIED\n",
    "############################################################\n",
    "\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = StockDataset(csv_path='train_raw.csv')\n",
    "\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset,\n",
    "                          batch_size=BATCH_SIZE,\n",
    "                          shuffle=True,\n",
    "                          num_workers=0)\n",
    "\n",
    "\n",
    "valid_dataset = StockDataset(csv_path='val_raw.csv')\n",
    "\n",
    "valid_loader = DataLoader(dataset=valid_dataset,\n",
    "                          batch_size=BATCH_SIZE,\n",
    "                          shuffle=True,\n",
    "                          num_workers=0)\n",
    "\n",
    "test_dataset = StockDataset(csv_path='test_raw.csv')\n",
    "\n",
    "test_loader = DataLoader(dataset=test_dataset,\n",
    "                         batch_size=BATCH_SIZE,\n",
    "                         shuffle=True,\n",
    "                         num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below just checks if the dataset can be loaded correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | Batch index: 0 | Batch size: 64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -2.5601e-01,\n",
       "          2.0000e+00,  2.0001e+00],\n",
       "        [ 2.3606e+08, -6.8669e-01,  5.5899e+07,  ...,  1.2693e-01,\n",
       "          2.7610e+01,  2.8863e+01],\n",
       "        [ 8.1570e+09, -4.4736e-02,  1.5450e+09,  ..., -3.1968e-02,\n",
       "          8.2200e+00,  8.8406e+00],\n",
       "        ...,\n",
       "        [ 1.3804e+07,  1.0000e+00,  8.0160e+06,  ...,  1.8480e-01,\n",
       "          9.2600e+00,  9.8709e+00],\n",
       "        [ 1.0377e+07,  1.2953e-02,  8.6582e+06,  ..., -1.3073e-01,\n",
       "          3.4500e+00,  3.6882e+00],\n",
       "        [ 3.3802e+08,  3.8812e-02,  2.8956e+08,  ..., -1.5701e-02,\n",
       "          9.2400e+00,  1.2038e+01]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([8., 3., 0., 1., 3., 0., 2., 5., 1., 5., 6., 4., 4., 3., 0., 8., 2., 1.,\n",
       "        1., 4., 2., 7., 3., 4., 8., 7., 6., 1., 8., 6., 1., 3., 2., 8., 5., 2.,\n",
       "        6., 7., 4., 4., 0., 2., 3., 5., 6., 1., 5., 3., 6., 0., 2., 1., 8., 7.,\n",
       "        3., 5., 3., 7., 5., 6., 7., 0., 2., 5.])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "break minibatch for-loop\n",
      "Epoch: 2 | Batch index: 0 | Batch size: 64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.8803e+10, -2.7672e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "          3.1250e+01,  3.3071e+01],\n",
       "        [ 1.9880e+07,  3.8941e-01,  0.0000e+00,  ..., -5.7223e-01,\n",
       "          3.1900e+00,  3.6149e+00],\n",
       "        [ 2.3950e+09,  1.3542e-02,  1.9880e+09,  ...,  9.5023e-02,\n",
       "          2.0580e+01,  2.2332e+01],\n",
       "        ...,\n",
       "        [ 3.5170e+06,  1.2574e+00,  2.3360e+06,  ...,  0.0000e+00,\n",
       "          1.0600e+00,  0.0000e+00],\n",
       "        [ 1.0711e+09, -1.1028e-01,  6.1719e+08,  ..., -3.3579e-02,\n",
       "          5.1400e+01,  5.3680e+01],\n",
       "        [ 5.5365e+08, -7.2477e-03,  2.8683e+08,  ...,  1.2746e-01,\n",
       "          2.2790e+01,  2.4587e+01]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([7., 5., 7., 1., 3., 4., 8., 8., 5., 6., 2., 1., 7., 5., 2., 8., 4., 1.,\n",
       "        5., 3., 8., 7., 7., 1., 7., 0., 3., 4., 2., 0., 3., 1., 7., 7., 5., 5.,\n",
       "        6., 8., 5., 5., 7., 6., 2., 0., 7., 0., 3., 0., 8., 2., 8., 4., 6., 7.,\n",
       "        3., 3., 2., 5., 0., 6., 1., 1., 2., 1.])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "break minibatch for-loop\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "num_epochs = 2\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    for batch_idx, (x, y) in enumerate(train_loader):\n",
    "        \n",
    "        print('Epoch:', epoch+1, end='')\n",
    "        print(' | Batch index:', batch_idx, end='')\n",
    "        print(' | Batch size:', y.size()[0])\n",
    "        \n",
    "        x = x.to(DEVICE)\n",
    "        y = y.to(DEVICE)\n",
    "        display(x)\n",
    "        display(y)\n",
    "        print('break minibatch for-loop')\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you get an error, make sure the `png-files` folder is unzipped and it the same directory as this notebook!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multilayer Perceptron Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below contains the multi-layer perceptron model. This is the main section where you want to make changes to the architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = 143"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################\n",
    "# THIS CELL CAN BE MODIFIED\n",
    "############################################################\n",
    "\n",
    "\n",
    "class MLP(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, num_features, num_hidden_1, num_hidden_2, num_hidden_3, num_classes):\n",
    "        super(MLP, self).__init__()\n",
    "        \n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        ### ADD ADDITIONAL LAYERS BELOW IF YOU LIKE\n",
    "        self.linear_1 = torch.nn.Linear(num_features, num_hidden_1)\n",
    "        self.linear_1_bn = torch.nn.BatchNorm1d(num_hidden_1)\n",
    "\n",
    "        self.linear_2 = torch.nn.Linear(num_hidden_1, num_hidden_2)\n",
    "        self.linear_2_bn = torch.nn.BatchNorm1d(num_hidden_2)\n",
    "\n",
    "        self.linear_3 = torch.nn.Linear(num_hidden_2, num_hidden_3)\n",
    "        self.linear_3_bn = torch.nn.BatchNorm1d(num_hidden_3)\n",
    "\n",
    "        self.linear_out = torch.nn.Linear(num_hidden_3, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        ### MAKE SURE YOU CONNECT THE LAYERS PROPERLY IF YOU CHANGED\n",
    "        ### ANYTHNG IN THE __init__ METHOD ABOVE       \n",
    "        out = self.linear_1(x)\n",
    "#         out = self.linear_1_bn(out)\n",
    "        out = torch.sigmoid(out)\n",
    "        out = F.dropout(out, p=0.5, training=self.training)\n",
    "        \n",
    "#         out = self.linear_2(out)\n",
    "#         out = self.linear_2_bn(out)\n",
    "#         out = torch.relu(out)\n",
    "#         out = F.dropout(out, p=0.5, training=self.training)\n",
    "        \n",
    "#         out = self.linear_3(out)\n",
    "#         out = self.linear_3_bn(out)\n",
    "#         out = torch.relu(out)\n",
    "#         out = F.dropout(out, p=0.5, training=self.training)\n",
    "\n",
    "        logits = self.linear_out(out)\n",
    "        probas = torch.sigmoid(logits)\n",
    "        return logits, probas\n",
    "        \n",
    "        \n",
    "#################################\n",
    "### Model Initialization\n",
    "#################################\n",
    "\n",
    "\n",
    "# the random seed makes sure that the random weight initialization\n",
    "# in the model is always the same.\n",
    "# In practice, some weights don't work well, and we may also want\n",
    "# to try different random seeds. In this homework, this is not\n",
    "# necessary.\n",
    "random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "\n",
    "### IF YOU CHANGED THE ARCHITECTURE ABOVE, MAKE SURE YOU \n",
    "### ACCOUNT FOR IT VIA THE PARAMETERS BELOW. I.e., if you\n",
    "### added a second hidden layer, you may want to add a\n",
    "### hidden_2 parameter here. Also you may want to play\n",
    "### with the number of hidden units.\n",
    "model = MLP(num_features=143,\n",
    "            num_hidden_1=100,\n",
    "            num_hidden_2=100,\n",
    "            num_hidden_3=100,\n",
    "            num_classes=1)\n",
    "\n",
    "model = model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################\n",
    "# THIS CELL CAN BE MODIFIED\n",
    "############################################################\n",
    "\n",
    "### For this homework, do not change the optimizer. However, you\n",
    "### likely want to experiment with the learning rate!\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.00001)\n",
    "###########################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################\n",
    "# THIS CELL CAN BE MODIFIED\n",
    "############################################################\n",
    "\n",
    "NUM_EPOCHS = 1000 # Please feel free to change\n",
    "############################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001/1000 | Batch 000/016 | Cost: 6.4526\n",
      "Epoch: 001/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.00 min\n",
      "Epoch: 002/1000 | Batch 000/016 | Cost: 6.1526\n",
      "Epoch: 002/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.00 min\n",
      "Epoch: 003/1000 | Batch 000/016 | Cost: 6.6207\n",
      "Epoch: 003/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.00 min\n",
      "Epoch: 004/1000 | Batch 000/016 | Cost: 6.9216\n",
      "Epoch: 004/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.00 min\n",
      "Epoch: 005/1000 | Batch 000/016 | Cost: 7.1703\n",
      "Epoch: 005/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.00 min\n",
      "Epoch: 006/1000 | Batch 000/016 | Cost: 6.9044\n",
      "Epoch: 006/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.00 min\n",
      "Epoch: 007/1000 | Batch 000/016 | Cost: 7.5263\n",
      "Epoch: 007/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.01 min\n",
      "Epoch: 008/1000 | Batch 000/016 | Cost: 5.1537\n",
      "Epoch: 008/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.01 min\n",
      "Epoch: 009/1000 | Batch 000/016 | Cost: 7.9210\n",
      "Epoch: 009/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.01 min\n",
      "Epoch: 010/1000 | Batch 000/016 | Cost: 7.4336\n",
      "Epoch: 010/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.01 min\n",
      "Epoch: 011/1000 | Batch 000/016 | Cost: 6.3965\n",
      "Epoch: 011/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.01 min\n",
      "Epoch: 012/1000 | Batch 000/016 | Cost: 7.2740\n",
      "Epoch: 012/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.01 min\n",
      "Epoch: 013/1000 | Batch 000/016 | Cost: 6.1012\n",
      "Epoch: 013/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.01 min\n",
      "Epoch: 014/1000 | Batch 000/016 | Cost: 5.2742\n",
      "Epoch: 014/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.01 min\n",
      "Epoch: 015/1000 | Batch 000/016 | Cost: 6.3060\n",
      "Epoch: 015/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.01 min\n",
      "Epoch: 016/1000 | Batch 000/016 | Cost: 6.6461\n",
      "Epoch: 016/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.01 min\n",
      "Epoch: 017/1000 | Batch 000/016 | Cost: 5.7021\n",
      "Epoch: 017/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.01 min\n",
      "Epoch: 018/1000 | Batch 000/016 | Cost: 7.3521\n",
      "Epoch: 018/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.01 min\n",
      "Epoch: 019/1000 | Batch 000/016 | Cost: 6.4385\n",
      "Epoch: 019/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.01 min\n",
      "Epoch: 020/1000 | Batch 000/016 | Cost: 5.1215\n",
      "Epoch: 020/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.02 min\n",
      "Epoch: 021/1000 | Batch 000/016 | Cost: 6.4464\n",
      "Epoch: 021/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.02 min\n",
      "Epoch: 022/1000 | Batch 000/016 | Cost: 6.1371\n",
      "Epoch: 022/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.02 min\n",
      "Epoch: 023/1000 | Batch 000/016 | Cost: 6.2158\n",
      "Epoch: 023/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.02 min\n",
      "Epoch: 024/1000 | Batch 000/016 | Cost: 6.1342\n",
      "Epoch: 024/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.02 min\n",
      "Epoch: 025/1000 | Batch 000/016 | Cost: 7.9982\n",
      "Epoch: 025/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.02 min\n",
      "Epoch: 026/1000 | Batch 000/016 | Cost: 5.6930\n",
      "Epoch: 026/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.02 min\n",
      "Epoch: 027/1000 | Batch 000/016 | Cost: 6.4468\n",
      "Epoch: 027/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.02 min\n",
      "Epoch: 028/1000 | Batch 000/016 | Cost: 5.2155\n",
      "Epoch: 028/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.02 min\n",
      "Epoch: 029/1000 | Batch 000/016 | Cost: 7.8636\n",
      "Epoch: 029/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.02 min\n",
      "Epoch: 030/1000 | Batch 000/016 | Cost: 6.7210\n",
      "Epoch: 030/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.02 min\n",
      "Epoch: 031/1000 | Batch 000/016 | Cost: 6.3156\n",
      "Epoch: 031/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.02 min\n",
      "Epoch: 032/1000 | Batch 000/016 | Cost: 7.0909\n",
      "Epoch: 032/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.02 min\n",
      "Epoch: 033/1000 | Batch 000/016 | Cost: 5.9591\n",
      "Epoch: 033/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.03 min\n",
      "Epoch: 034/1000 | Batch 000/016 | Cost: 7.1392\n",
      "Epoch: 034/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.03 min\n",
      "Epoch: 035/1000 | Batch 000/016 | Cost: 6.5610\n",
      "Epoch: 035/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.03 min\n",
      "Epoch: 036/1000 | Batch 000/016 | Cost: 6.1917\n",
      "Epoch: 036/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.03 min\n",
      "Epoch: 037/1000 | Batch 000/016 | Cost: 5.8353\n",
      "Epoch: 037/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.03 min\n",
      "Epoch: 038/1000 | Batch 000/016 | Cost: 6.7536\n",
      "Epoch: 038/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.03 min\n",
      "Epoch: 039/1000 | Batch 000/016 | Cost: 6.0647\n",
      "Epoch: 039/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.03 min\n",
      "Epoch: 040/1000 | Batch 000/016 | Cost: 6.6682\n",
      "Epoch: 040/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.03 min\n",
      "Epoch: 041/1000 | Batch 000/016 | Cost: 6.1736\n",
      "Epoch: 041/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.03 min\n",
      "Epoch: 042/1000 | Batch 000/016 | Cost: 6.6029\n",
      "Epoch: 042/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.03 min\n",
      "Epoch: 043/1000 | Batch 000/016 | Cost: 5.5033\n",
      "Epoch: 043/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.03 min\n",
      "Epoch: 044/1000 | Batch 000/016 | Cost: 6.7023\n",
      "Epoch: 044/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.03 min\n",
      "Epoch: 045/1000 | Batch 000/016 | Cost: 6.0840\n",
      "Epoch: 045/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.03 min\n",
      "Epoch: 046/1000 | Batch 000/016 | Cost: 6.7786\n",
      "Epoch: 046/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.04 min\n",
      "Epoch: 047/1000 | Batch 000/016 | Cost: 5.4812\n",
      "Epoch: 047/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.04 min\n",
      "Epoch: 048/1000 | Batch 000/016 | Cost: 5.5719\n",
      "Epoch: 048/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.04 min\n",
      "Epoch: 049/1000 | Batch 000/016 | Cost: 5.8048\n",
      "Epoch: 049/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.04 min\n",
      "Epoch: 050/1000 | Batch 000/016 | Cost: 6.2324\n",
      "Epoch: 050/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.04 min\n",
      "Epoch: 051/1000 | Batch 000/016 | Cost: 4.5105\n",
      "Epoch: 051/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.04 min\n",
      "Epoch: 052/1000 | Batch 000/016 | Cost: 8.0523\n",
      "Epoch: 052/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.04 min\n",
      "Epoch: 053/1000 | Batch 000/016 | Cost: 6.6432\n",
      "Epoch: 053/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.04 min\n",
      "Epoch: 054/1000 | Batch 000/016 | Cost: 8.1567\n",
      "Epoch: 054/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.04 min\n",
      "Epoch: 055/1000 | Batch 000/016 | Cost: 5.4747\n",
      "Epoch: 055/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.04 min\n",
      "Epoch: 056/1000 | Batch 000/016 | Cost: 6.7766\n",
      "Epoch: 056/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.04 min\n",
      "Epoch: 057/1000 | Batch 000/016 | Cost: 6.4486\n",
      "Epoch: 057/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.04 min\n",
      "Epoch: 058/1000 | Batch 000/016 | Cost: 6.2266\n",
      "Epoch: 058/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.04 min\n",
      "Epoch: 059/1000 | Batch 000/016 | Cost: 7.8041\n",
      "Epoch: 059/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.05 min\n",
      "Epoch: 060/1000 | Batch 000/016 | Cost: 5.1401\n",
      "Epoch: 060/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.05 min\n",
      "Epoch: 061/1000 | Batch 000/016 | Cost: 5.6588\n",
      "Epoch: 061/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.05 min\n",
      "Epoch: 062/1000 | Batch 000/016 | Cost: 6.2059\n",
      "Epoch: 062/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.05 min\n",
      "Epoch: 063/1000 | Batch 000/016 | Cost: 6.2396\n",
      "Epoch: 063/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.05 min\n",
      "Epoch: 064/1000 | Batch 000/016 | Cost: 6.9436\n",
      "Epoch: 064/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.05 min\n",
      "Epoch: 065/1000 | Batch 000/016 | Cost: 6.8287\n",
      "Epoch: 065/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.05 min\n",
      "Epoch: 066/1000 | Batch 000/016 | Cost: 6.5434\n",
      "Epoch: 066/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.05 min\n",
      "Epoch: 067/1000 | Batch 000/016 | Cost: 6.8074\n",
      "Epoch: 067/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.05 min\n",
      "Epoch: 068/1000 | Batch 000/016 | Cost: 6.8404\n",
      "Epoch: 068/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.05 min\n",
      "Epoch: 069/1000 | Batch 000/016 | Cost: 5.8527\n",
      "Epoch: 069/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.05 min\n",
      "Epoch: 070/1000 | Batch 000/016 | Cost: 5.6458\n",
      "Epoch: 070/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.05 min\n",
      "Epoch: 071/1000 | Batch 000/016 | Cost: 5.5358\n",
      "Epoch: 071/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.05 min\n",
      "Epoch: 072/1000 | Batch 000/016 | Cost: 7.6310\n",
      "Epoch: 072/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.06 min\n",
      "Epoch: 073/1000 | Batch 000/016 | Cost: 5.6032\n",
      "Epoch: 073/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.06 min\n",
      "Epoch: 074/1000 | Batch 000/016 | Cost: 6.7129\n",
      "Epoch: 074/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.06 min\n",
      "Epoch: 075/1000 | Batch 000/016 | Cost: 6.3668\n",
      "Epoch: 075/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.06 min\n",
      "Epoch: 076/1000 | Batch 000/016 | Cost: 6.4853\n",
      "Epoch: 076/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.06 min\n",
      "Epoch: 077/1000 | Batch 000/016 | Cost: 6.5061\n",
      "Epoch: 077/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.06 min\n",
      "Epoch: 078/1000 | Batch 000/016 | Cost: 6.1074\n",
      "Epoch: 078/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.06 min\n",
      "Epoch: 079/1000 | Batch 000/016 | Cost: 7.6819\n",
      "Epoch: 079/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.06 min\n",
      "Epoch: 080/1000 | Batch 000/016 | Cost: 6.3118\n",
      "Epoch: 080/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.06 min\n",
      "Epoch: 081/1000 | Batch 000/016 | Cost: 7.5011\n",
      "Epoch: 081/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.06 min\n",
      "Epoch: 082/1000 | Batch 000/016 | Cost: 6.6373\n",
      "Epoch: 082/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.06 min\n",
      "Epoch: 083/1000 | Batch 000/016 | Cost: 5.8151\n",
      "Epoch: 083/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.06 min\n",
      "Epoch: 084/1000 | Batch 000/016 | Cost: 5.9814\n",
      "Epoch: 084/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.06 min\n",
      "Epoch: 085/1000 | Batch 000/016 | Cost: 5.5096\n",
      "Epoch: 085/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.06 min\n",
      "Epoch: 086/1000 | Batch 000/016 | Cost: 7.2748\n",
      "Epoch: 086/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.07 min\n",
      "Epoch: 087/1000 | Batch 000/016 | Cost: 6.4242\n",
      "Epoch: 087/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.07 min\n",
      "Epoch: 088/1000 | Batch 000/016 | Cost: 6.1353\n",
      "Epoch: 088/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.07 min\n",
      "Epoch: 089/1000 | Batch 000/016 | Cost: 6.2601\n",
      "Epoch: 089/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.07 min\n",
      "Epoch: 090/1000 | Batch 000/016 | Cost: 6.5466\n",
      "Epoch: 090/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.07 min\n",
      "Epoch: 091/1000 | Batch 000/016 | Cost: 6.8171\n",
      "Epoch: 091/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.07 min\n",
      "Epoch: 092/1000 | Batch 000/016 | Cost: 6.4075\n",
      "Epoch: 092/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.07 min\n",
      "Epoch: 093/1000 | Batch 000/016 | Cost: 6.5727\n",
      "Epoch: 093/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.07 min\n",
      "Epoch: 094/1000 | Batch 000/016 | Cost: 6.2901\n",
      "Epoch: 094/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.07 min\n",
      "Epoch: 095/1000 | Batch 000/016 | Cost: 7.0352\n",
      "Epoch: 095/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.07 min\n",
      "Epoch: 096/1000 | Batch 000/016 | Cost: 4.5438\n",
      "Epoch: 096/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.07 min\n",
      "Epoch: 097/1000 | Batch 000/016 | Cost: 6.5088\n",
      "Epoch: 097/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.07 min\n",
      "Epoch: 098/1000 | Batch 000/016 | Cost: 6.5997\n",
      "Epoch: 098/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.07 min\n",
      "Epoch: 099/1000 | Batch 000/016 | Cost: 6.8947\n",
      "Epoch: 099/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.07 min\n",
      "Epoch: 100/1000 | Batch 000/016 | Cost: 5.9145\n",
      "Epoch: 100/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.08 min\n",
      "Epoch: 101/1000 | Batch 000/016 | Cost: 6.6431\n",
      "Epoch: 101/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.08 min\n",
      "Epoch: 102/1000 | Batch 000/016 | Cost: 6.1598\n",
      "Epoch: 102/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.08 min\n",
      "Epoch: 103/1000 | Batch 000/016 | Cost: 5.4746\n",
      "Epoch: 103/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.08 min\n",
      "Epoch: 104/1000 | Batch 000/016 | Cost: 7.0044\n",
      "Epoch: 104/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.08 min\n",
      "Epoch: 105/1000 | Batch 000/016 | Cost: 6.1405\n",
      "Epoch: 105/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.08 min\n",
      "Epoch: 106/1000 | Batch 000/016 | Cost: 5.6722\n",
      "Epoch: 106/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.08 min\n",
      "Epoch: 107/1000 | Batch 000/016 | Cost: 7.1428\n",
      "Epoch: 107/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.08 min\n",
      "Epoch: 108/1000 | Batch 000/016 | Cost: 5.5960\n",
      "Epoch: 108/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.08 min\n",
      "Epoch: 109/1000 | Batch 000/016 | Cost: 5.7826\n",
      "Epoch: 109/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.08 min\n",
      "Epoch: 110/1000 | Batch 000/016 | Cost: 5.3213\n",
      "Epoch: 110/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.08 min\n",
      "Epoch: 111/1000 | Batch 000/016 | Cost: 6.2224\n",
      "Epoch: 111/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.08 min\n",
      "Epoch: 112/1000 | Batch 000/016 | Cost: 4.8983\n",
      "Epoch: 112/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.08 min\n",
      "Epoch: 113/1000 | Batch 000/016 | Cost: 5.9373\n",
      "Epoch: 113/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.09 min\n",
      "Epoch: 114/1000 | Batch 000/016 | Cost: 5.8905\n",
      "Epoch: 114/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.09 min\n",
      "Epoch: 115/1000 | Batch 000/016 | Cost: 4.7544\n",
      "Epoch: 115/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.09 min\n",
      "Epoch: 116/1000 | Batch 000/016 | Cost: 8.0762\n",
      "Epoch: 116/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.09 min\n",
      "Epoch: 117/1000 | Batch 000/016 | Cost: 6.3068\n",
      "Epoch: 117/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.09 min\n",
      "Epoch: 118/1000 | Batch 000/016 | Cost: 6.6450\n",
      "Epoch: 118/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.09 min\n",
      "Epoch: 119/1000 | Batch 000/016 | Cost: 6.4648\n",
      "Epoch: 119/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.09 min\n",
      "Epoch: 120/1000 | Batch 000/016 | Cost: 7.2584\n",
      "Epoch: 120/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.09 min\n",
      "Epoch: 121/1000 | Batch 000/016 | Cost: 6.1249\n",
      "Epoch: 121/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.09 min\n",
      "Epoch: 122/1000 | Batch 000/016 | Cost: 6.5202\n",
      "Epoch: 122/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.09 min\n",
      "Epoch: 123/1000 | Batch 000/016 | Cost: 7.7799\n",
      "Epoch: 123/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.09 min\n",
      "Epoch: 124/1000 | Batch 000/016 | Cost: 7.0951\n",
      "Epoch: 124/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.09 min\n",
      "Epoch: 125/1000 | Batch 000/016 | Cost: 6.2262\n",
      "Epoch: 125/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.09 min\n",
      "Epoch: 126/1000 | Batch 000/016 | Cost: 6.9329\n",
      "Epoch: 126/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.09 min\n",
      "Epoch: 127/1000 | Batch 000/016 | Cost: 6.5388\n",
      "Epoch: 127/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.10 min\n",
      "Epoch: 128/1000 | Batch 000/016 | Cost: 7.1459\n",
      "Epoch: 128/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.10 min\n",
      "Epoch: 129/1000 | Batch 000/016 | Cost: 6.1280\n",
      "Epoch: 129/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.10 min\n",
      "Epoch: 130/1000 | Batch 000/016 | Cost: 8.4609\n",
      "Epoch: 130/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.10 min\n",
      "Epoch: 131/1000 | Batch 000/016 | Cost: 6.3339\n",
      "Epoch: 131/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.10 min\n",
      "Epoch: 132/1000 | Batch 000/016 | Cost: 6.8382\n",
      "Epoch: 132/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.10 min\n",
      "Epoch: 133/1000 | Batch 000/016 | Cost: 5.8185\n",
      "Epoch: 133/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.10 min\n",
      "Epoch: 134/1000 | Batch 000/016 | Cost: 6.8027\n",
      "Epoch: 134/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.10 min\n",
      "Epoch: 135/1000 | Batch 000/016 | Cost: 6.0693\n",
      "Epoch: 135/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.10 min\n",
      "Epoch: 136/1000 | Batch 000/016 | Cost: 4.9894\n",
      "Epoch: 136/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.10 min\n",
      "Epoch: 137/1000 | Batch 000/016 | Cost: 6.8052\n",
      "Epoch: 137/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.10 min\n",
      "Epoch: 138/1000 | Batch 000/016 | Cost: 7.4069\n",
      "Epoch: 138/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.10 min\n",
      "Epoch: 139/1000 | Batch 000/016 | Cost: 5.9514\n",
      "Epoch: 139/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.10 min\n",
      "Epoch: 140/1000 | Batch 000/016 | Cost: 6.0315\n",
      "Epoch: 140/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.11 min\n",
      "Epoch: 141/1000 | Batch 000/016 | Cost: 5.8889\n",
      "Epoch: 141/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.11 min\n",
      "Epoch: 142/1000 | Batch 000/016 | Cost: 6.0793\n",
      "Epoch: 142/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.11 min\n",
      "Epoch: 143/1000 | Batch 000/016 | Cost: 7.5638\n",
      "Epoch: 143/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.11 min\n",
      "Epoch: 144/1000 | Batch 000/016 | Cost: 6.0758\n",
      "Epoch: 144/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.11 min\n",
      "Epoch: 145/1000 | Batch 000/016 | Cost: 6.4914\n",
      "Epoch: 145/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.11 min\n",
      "Epoch: 146/1000 | Batch 000/016 | Cost: 6.6612\n",
      "Epoch: 146/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.11 min\n",
      "Epoch: 147/1000 | Batch 000/016 | Cost: 7.5847\n",
      "Epoch: 147/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.11 min\n",
      "Epoch: 148/1000 | Batch 000/016 | Cost: 5.8224\n",
      "Epoch: 148/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.11 min\n",
      "Epoch: 149/1000 | Batch 000/016 | Cost: 6.3164\n",
      "Epoch: 149/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.11 min\n",
      "Epoch: 150/1000 | Batch 000/016 | Cost: 6.0747\n",
      "Epoch: 150/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.11 min\n",
      "Epoch: 151/1000 | Batch 000/016 | Cost: 5.7478\n",
      "Epoch: 151/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.11 min\n",
      "Epoch: 152/1000 | Batch 000/016 | Cost: 7.1697\n",
      "Epoch: 152/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.11 min\n",
      "Epoch: 153/1000 | Batch 000/016 | Cost: 6.8040\n",
      "Epoch: 153/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.12 min\n",
      "Epoch: 154/1000 | Batch 000/016 | Cost: 5.7766\n",
      "Epoch: 154/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.12 min\n",
      "Epoch: 155/1000 | Batch 000/016 | Cost: 5.2178\n",
      "Epoch: 155/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.12 min\n",
      "Epoch: 156/1000 | Batch 000/016 | Cost: 7.3289\n",
      "Epoch: 156/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.12 min\n",
      "Epoch: 157/1000 | Batch 000/016 | Cost: 7.0144\n",
      "Epoch: 157/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.12 min\n",
      "Epoch: 158/1000 | Batch 000/016 | Cost: 5.9371\n",
      "Epoch: 158/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.12 min\n",
      "Epoch: 159/1000 | Batch 000/016 | Cost: 6.2225\n",
      "Epoch: 159/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.12 min\n",
      "Epoch: 160/1000 | Batch 000/016 | Cost: 6.4636\n",
      "Epoch: 160/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.12 min\n",
      "Epoch: 161/1000 | Batch 000/016 | Cost: 7.2732\n",
      "Epoch: 161/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.12 min\n",
      "Epoch: 162/1000 | Batch 000/016 | Cost: 7.7867\n",
      "Epoch: 162/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.12 min\n",
      "Epoch: 163/1000 | Batch 000/016 | Cost: 6.1099\n",
      "Epoch: 163/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.12 min\n",
      "Epoch: 164/1000 | Batch 000/016 | Cost: 6.7822\n",
      "Epoch: 164/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.12 min\n",
      "Epoch: 165/1000 | Batch 000/016 | Cost: 6.5412\n",
      "Epoch: 165/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.12 min\n",
      "Epoch: 166/1000 | Batch 000/016 | Cost: 6.8575\n",
      "Epoch: 166/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.13 min\n",
      "Epoch: 167/1000 | Batch 000/016 | Cost: 6.8838\n",
      "Epoch: 167/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.13 min\n",
      "Epoch: 168/1000 | Batch 000/016 | Cost: 8.7587\n",
      "Epoch: 168/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.13 min\n",
      "Epoch: 169/1000 | Batch 000/016 | Cost: 7.0547\n",
      "Epoch: 169/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.13 min\n",
      "Epoch: 170/1000 | Batch 000/016 | Cost: 6.2574\n",
      "Epoch: 170/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.13 min\n",
      "Epoch: 171/1000 | Batch 000/016 | Cost: 5.6350\n",
      "Epoch: 171/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.13 min\n",
      "Epoch: 172/1000 | Batch 000/016 | Cost: 6.9326\n",
      "Epoch: 172/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.13 min\n",
      "Epoch: 173/1000 | Batch 000/016 | Cost: 6.0418\n",
      "Epoch: 173/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.13 min\n",
      "Epoch: 174/1000 | Batch 000/016 | Cost: 8.0864\n",
      "Epoch: 174/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.13 min\n",
      "Epoch: 175/1000 | Batch 000/016 | Cost: 7.6587\n",
      "Epoch: 175/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.13 min\n",
      "Epoch: 176/1000 | Batch 000/016 | Cost: 6.6919\n",
      "Epoch: 176/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.13 min\n",
      "Epoch: 177/1000 | Batch 000/016 | Cost: 6.3879\n",
      "Epoch: 177/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.13 min\n",
      "Epoch: 178/1000 | Batch 000/016 | Cost: 5.1390\n",
      "Epoch: 178/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.13 min\n",
      "Epoch: 179/1000 | Batch 000/016 | Cost: 6.4041\n",
      "Epoch: 179/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.14 min\n",
      "Epoch: 180/1000 | Batch 000/016 | Cost: 6.8396\n",
      "Epoch: 180/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.14 min\n",
      "Epoch: 181/1000 | Batch 000/016 | Cost: 6.5663\n",
      "Epoch: 181/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.14 min\n",
      "Epoch: 182/1000 | Batch 000/016 | Cost: 6.9299\n",
      "Epoch: 182/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.14 min\n",
      "Epoch: 183/1000 | Batch 000/016 | Cost: 6.6037\n",
      "Epoch: 183/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.14 min\n",
      "Epoch: 184/1000 | Batch 000/016 | Cost: 7.0948\n",
      "Epoch: 184/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.14 min\n",
      "Epoch: 185/1000 | Batch 000/016 | Cost: 5.7185\n",
      "Epoch: 185/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.14 min\n",
      "Epoch: 186/1000 | Batch 000/016 | Cost: 5.6290\n",
      "Epoch: 186/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.14 min\n",
      "Epoch: 187/1000 | Batch 000/016 | Cost: 6.8785\n",
      "Epoch: 187/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.14 min\n",
      "Epoch: 188/1000 | Batch 000/016 | Cost: 6.9475\n",
      "Epoch: 188/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.14 min\n",
      "Epoch: 189/1000 | Batch 000/016 | Cost: 7.1093\n",
      "Epoch: 189/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.14 min\n",
      "Epoch: 190/1000 | Batch 000/016 | Cost: 6.8911\n",
      "Epoch: 190/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.14 min\n",
      "Epoch: 191/1000 | Batch 000/016 | Cost: 7.2913\n",
      "Epoch: 191/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.14 min\n",
      "Epoch: 192/1000 | Batch 000/016 | Cost: 6.6898\n",
      "Epoch: 192/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.15 min\n",
      "Epoch: 193/1000 | Batch 000/016 | Cost: 7.1793\n",
      "Epoch: 193/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.15 min\n",
      "Epoch: 194/1000 | Batch 000/016 | Cost: 7.0216\n",
      "Epoch: 194/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.15 min\n",
      "Epoch: 195/1000 | Batch 000/016 | Cost: 6.6020\n",
      "Epoch: 195/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.15 min\n",
      "Epoch: 196/1000 | Batch 000/016 | Cost: 6.2532\n",
      "Epoch: 196/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.15 min\n",
      "Epoch: 197/1000 | Batch 000/016 | Cost: 6.5669\n",
      "Epoch: 197/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.15 min\n",
      "Epoch: 198/1000 | Batch 000/016 | Cost: 6.7012\n",
      "Epoch: 198/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.15 min\n",
      "Epoch: 199/1000 | Batch 000/016 | Cost: 7.5300\n",
      "Epoch: 199/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.15 min\n",
      "Epoch: 200/1000 | Batch 000/016 | Cost: 5.9094\n",
      "Epoch: 200/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.15 min\n",
      "Epoch: 201/1000 | Batch 000/016 | Cost: 6.8840\n",
      "Epoch: 201/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.15 min\n",
      "Epoch: 202/1000 | Batch 000/016 | Cost: 7.6978\n",
      "Epoch: 202/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.15 min\n",
      "Epoch: 203/1000 | Batch 000/016 | Cost: 6.5950\n",
      "Epoch: 203/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.15 min\n",
      "Epoch: 204/1000 | Batch 000/016 | Cost: 6.4703\n",
      "Epoch: 204/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.15 min\n",
      "Epoch: 205/1000 | Batch 000/016 | Cost: 8.4613\n",
      "Epoch: 205/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.16 min\n",
      "Epoch: 206/1000 | Batch 000/016 | Cost: 8.1468\n",
      "Epoch: 206/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.16 min\n",
      "Epoch: 207/1000 | Batch 000/016 | Cost: 5.0900\n",
      "Epoch: 207/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.16 min\n",
      "Epoch: 208/1000 | Batch 000/016 | Cost: 6.4473\n",
      "Epoch: 208/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.16 min\n",
      "Epoch: 209/1000 | Batch 000/016 | Cost: 6.2792\n",
      "Epoch: 209/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.16 min\n",
      "Epoch: 210/1000 | Batch 000/016 | Cost: 7.6307\n",
      "Epoch: 210/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.16 min\n",
      "Epoch: 211/1000 | Batch 000/016 | Cost: 7.2997\n",
      "Epoch: 211/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.16 min\n",
      "Epoch: 212/1000 | Batch 000/016 | Cost: 7.8819\n",
      "Epoch: 212/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.16 min\n",
      "Epoch: 213/1000 | Batch 000/016 | Cost: 5.9227\n",
      "Epoch: 213/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.16 min\n",
      "Epoch: 214/1000 | Batch 000/016 | Cost: 5.9428\n",
      "Epoch: 214/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.16 min\n",
      "Epoch: 215/1000 | Batch 000/016 | Cost: 6.5103\n",
      "Epoch: 215/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.16 min\n",
      "Epoch: 216/1000 | Batch 000/016 | Cost: 5.9777\n",
      "Epoch: 216/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.16 min\n",
      "Epoch: 217/1000 | Batch 000/016 | Cost: 6.9161\n",
      "Epoch: 217/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.16 min\n",
      "Epoch: 218/1000 | Batch 000/016 | Cost: 6.3926\n",
      "Epoch: 218/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.17 min\n",
      "Epoch: 219/1000 | Batch 000/016 | Cost: 5.8238\n",
      "Epoch: 219/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.17 min\n",
      "Epoch: 220/1000 | Batch 000/016 | Cost: 6.3989\n",
      "Epoch: 220/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.17 min\n",
      "Epoch: 221/1000 | Batch 000/016 | Cost: 7.7938\n",
      "Epoch: 221/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.17 min\n",
      "Epoch: 222/1000 | Batch 000/016 | Cost: 6.6909\n",
      "Epoch: 222/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.17 min\n",
      "Epoch: 223/1000 | Batch 000/016 | Cost: 6.4530\n",
      "Epoch: 223/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.17 min\n",
      "Epoch: 224/1000 | Batch 000/016 | Cost: 7.1885\n",
      "Epoch: 224/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.17 min\n",
      "Epoch: 225/1000 | Batch 000/016 | Cost: 6.0024\n",
      "Epoch: 225/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.17 min\n",
      "Epoch: 226/1000 | Batch 000/016 | Cost: 5.8330\n",
      "Epoch: 226/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.17 min\n",
      "Epoch: 227/1000 | Batch 000/016 | Cost: 5.8335\n",
      "Epoch: 227/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.17 min\n",
      "Epoch: 228/1000 | Batch 000/016 | Cost: 6.2516\n",
      "Epoch: 228/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.17 min\n",
      "Epoch: 229/1000 | Batch 000/016 | Cost: 5.6289\n",
      "Epoch: 229/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.17 min\n",
      "Epoch: 230/1000 | Batch 000/016 | Cost: 7.5001\n",
      "Epoch: 230/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.17 min\n",
      "Epoch: 231/1000 | Batch 000/016 | Cost: 5.6904\n",
      "Epoch: 231/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.17 min\n",
      "Epoch: 232/1000 | Batch 000/016 | Cost: 6.9654\n",
      "Epoch: 232/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.18 min\n",
      "Epoch: 233/1000 | Batch 000/016 | Cost: 6.3584\n",
      "Epoch: 233/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.18 min\n",
      "Epoch: 234/1000 | Batch 000/016 | Cost: 6.4148\n",
      "Epoch: 234/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.18 min\n",
      "Epoch: 235/1000 | Batch 000/016 | Cost: 6.1750\n",
      "Epoch: 235/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.18 min\n",
      "Epoch: 236/1000 | Batch 000/016 | Cost: 5.7162\n",
      "Epoch: 236/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.18 min\n",
      "Epoch: 237/1000 | Batch 000/016 | Cost: 8.0034\n",
      "Epoch: 237/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.18 min\n",
      "Epoch: 238/1000 | Batch 000/016 | Cost: 5.8227\n",
      "Epoch: 238/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.18 min\n",
      "Epoch: 239/1000 | Batch 000/016 | Cost: 6.1059\n",
      "Epoch: 239/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.18 min\n",
      "Epoch: 240/1000 | Batch 000/016 | Cost: 6.5116\n",
      "Epoch: 240/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.18 min\n",
      "Epoch: 241/1000 | Batch 000/016 | Cost: 7.3294\n",
      "Epoch: 241/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.18 min\n",
      "Epoch: 242/1000 | Batch 000/016 | Cost: 6.6758\n",
      "Epoch: 242/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.18 min\n",
      "Epoch: 243/1000 | Batch 000/016 | Cost: 5.1423\n",
      "Epoch: 243/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.18 min\n",
      "Epoch: 244/1000 | Batch 000/016 | Cost: 7.1085\n",
      "Epoch: 244/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.19 min\n",
      "Epoch: 245/1000 | Batch 000/016 | Cost: 6.1104\n",
      "Epoch: 245/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.19 min\n",
      "Epoch: 246/1000 | Batch 000/016 | Cost: 6.8341\n",
      "Epoch: 246/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.19 min\n",
      "Epoch: 247/1000 | Batch 000/016 | Cost: 6.8407\n",
      "Epoch: 247/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.19 min\n",
      "Epoch: 248/1000 | Batch 000/016 | Cost: 6.1508\n",
      "Epoch: 248/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.19 min\n",
      "Epoch: 249/1000 | Batch 000/016 | Cost: 7.2950\n",
      "Epoch: 249/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.19 min\n",
      "Epoch: 250/1000 | Batch 000/016 | Cost: 6.1584\n",
      "Epoch: 250/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.19 min\n",
      "Epoch: 251/1000 | Batch 000/016 | Cost: 7.0421\n",
      "Epoch: 251/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.19 min\n",
      "Epoch: 252/1000 | Batch 000/016 | Cost: 5.3922\n",
      "Epoch: 252/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.19 min\n",
      "Epoch: 253/1000 | Batch 000/016 | Cost: 5.3821\n",
      "Epoch: 253/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.19 min\n",
      "Epoch: 254/1000 | Batch 000/016 | Cost: 5.1783\n",
      "Epoch: 254/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.19 min\n",
      "Epoch: 255/1000 | Batch 000/016 | Cost: 6.5882\n",
      "Epoch: 255/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.19 min\n",
      "Epoch: 256/1000 | Batch 000/016 | Cost: 6.7437\n",
      "Epoch: 256/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.19 min\n",
      "Epoch: 257/1000 | Batch 000/016 | Cost: 6.3478\n",
      "Epoch: 257/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.20 min\n",
      "Epoch: 258/1000 | Batch 000/016 | Cost: 7.1507\n",
      "Epoch: 258/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.20 min\n",
      "Epoch: 259/1000 | Batch 000/016 | Cost: 5.6389\n",
      "Epoch: 259/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.20 min\n",
      "Epoch: 260/1000 | Batch 000/016 | Cost: 5.8339\n",
      "Epoch: 260/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.20 min\n",
      "Epoch: 261/1000 | Batch 000/016 | Cost: 8.7676\n",
      "Epoch: 261/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.20 min\n",
      "Epoch: 262/1000 | Batch 000/016 | Cost: 7.3616\n",
      "Epoch: 262/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.20 min\n",
      "Epoch: 263/1000 | Batch 000/016 | Cost: 7.0467\n",
      "Epoch: 263/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.20 min\n",
      "Epoch: 264/1000 | Batch 000/016 | Cost: 6.1932\n",
      "Epoch: 264/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.20 min\n",
      "Epoch: 265/1000 | Batch 000/016 | Cost: 6.8089\n",
      "Epoch: 265/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.20 min\n",
      "Epoch: 266/1000 | Batch 000/016 | Cost: 5.0904\n",
      "Epoch: 266/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.20 min\n",
      "Epoch: 267/1000 | Batch 000/016 | Cost: 6.7428\n",
      "Epoch: 267/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.20 min\n",
      "Epoch: 268/1000 | Batch 000/016 | Cost: 6.5697\n",
      "Epoch: 268/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.20 min\n",
      "Epoch: 269/1000 | Batch 000/016 | Cost: 7.5838\n",
      "Epoch: 269/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.20 min\n",
      "Epoch: 270/1000 | Batch 000/016 | Cost: 6.8274\n",
      "Epoch: 270/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.21 min\n",
      "Epoch: 271/1000 | Batch 000/016 | Cost: 5.6581\n",
      "Epoch: 271/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.21 min\n",
      "Epoch: 272/1000 | Batch 000/016 | Cost: 4.7079\n",
      "Epoch: 272/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.21 min\n",
      "Epoch: 273/1000 | Batch 000/016 | Cost: 6.1448\n",
      "Epoch: 273/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.21 min\n",
      "Epoch: 274/1000 | Batch 000/016 | Cost: 6.0232\n",
      "Epoch: 274/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.21 min\n",
      "Epoch: 275/1000 | Batch 000/016 | Cost: 6.8284\n",
      "Epoch: 275/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.21 min\n",
      "Epoch: 276/1000 | Batch 000/016 | Cost: 6.1138\n",
      "Epoch: 276/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.21 min\n",
      "Epoch: 277/1000 | Batch 000/016 | Cost: 6.9554\n",
      "Epoch: 277/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.21 min\n",
      "Epoch: 278/1000 | Batch 000/016 | Cost: 6.0111\n",
      "Epoch: 278/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.21 min\n",
      "Epoch: 279/1000 | Batch 000/016 | Cost: 5.4812\n",
      "Epoch: 279/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.21 min\n",
      "Epoch: 280/1000 | Batch 000/016 | Cost: 7.6881\n",
      "Epoch: 280/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.21 min\n",
      "Epoch: 281/1000 | Batch 000/016 | Cost: 6.8595\n",
      "Epoch: 281/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.21 min\n",
      "Epoch: 282/1000 | Batch 000/016 | Cost: 6.7436\n",
      "Epoch: 282/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.22 min\n",
      "Epoch: 283/1000 | Batch 000/016 | Cost: 6.9902\n",
      "Epoch: 283/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.22 min\n",
      "Epoch: 284/1000 | Batch 000/016 | Cost: 5.6138\n",
      "Epoch: 284/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.22 min\n",
      "Epoch: 285/1000 | Batch 000/016 | Cost: 7.0458\n",
      "Epoch: 285/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.22 min\n",
      "Epoch: 286/1000 | Batch 000/016 | Cost: 6.1276\n",
      "Epoch: 286/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.22 min\n",
      "Epoch: 287/1000 | Batch 000/016 | Cost: 6.2379\n",
      "Epoch: 287/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.22 min\n",
      "Epoch: 288/1000 | Batch 000/016 | Cost: 6.2200\n",
      "Epoch: 288/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.22 min\n",
      "Epoch: 289/1000 | Batch 000/016 | Cost: 7.0517\n",
      "Epoch: 289/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.22 min\n",
      "Epoch: 290/1000 | Batch 000/016 | Cost: 6.8095\n",
      "Epoch: 290/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.22 min\n",
      "Epoch: 291/1000 | Batch 000/016 | Cost: 7.0327\n",
      "Epoch: 291/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.22 min\n",
      "Epoch: 292/1000 | Batch 000/016 | Cost: 6.0435\n",
      "Epoch: 292/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.22 min\n",
      "Epoch: 293/1000 | Batch 000/016 | Cost: 6.0516\n",
      "Epoch: 293/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.22 min\n",
      "Epoch: 294/1000 | Batch 000/016 | Cost: 6.1815\n",
      "Epoch: 294/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.23 min\n",
      "Epoch: 295/1000 | Batch 000/016 | Cost: 5.7881\n",
      "Epoch: 295/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.23 min\n",
      "Epoch: 296/1000 | Batch 000/016 | Cost: 7.0782\n",
      "Epoch: 296/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.23 min\n",
      "Epoch: 297/1000 | Batch 000/016 | Cost: 6.5456\n",
      "Epoch: 297/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.23 min\n",
      "Epoch: 298/1000 | Batch 000/016 | Cost: 6.6245\n",
      "Epoch: 298/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.23 min\n",
      "Epoch: 299/1000 | Batch 000/016 | Cost: 6.1319\n",
      "Epoch: 299/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.23 min\n",
      "Epoch: 300/1000 | Batch 000/016 | Cost: 7.8990\n",
      "Epoch: 300/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.23 min\n",
      "Epoch: 301/1000 | Batch 000/016 | Cost: 5.7751\n",
      "Epoch: 301/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.23 min\n",
      "Epoch: 302/1000 | Batch 000/016 | Cost: 5.8737\n",
      "Epoch: 302/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.23 min\n",
      "Epoch: 303/1000 | Batch 000/016 | Cost: 6.6607\n",
      "Epoch: 303/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.23 min\n",
      "Epoch: 304/1000 | Batch 000/016 | Cost: 5.5009\n",
      "Epoch: 304/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.23 min\n",
      "Epoch: 305/1000 | Batch 000/016 | Cost: 7.4789\n",
      "Epoch: 305/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.23 min\n",
      "Epoch: 306/1000 | Batch 000/016 | Cost: 6.9405\n",
      "Epoch: 306/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.24 min\n",
      "Epoch: 307/1000 | Batch 000/016 | Cost: 6.7092\n",
      "Epoch: 307/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.24 min\n",
      "Epoch: 308/1000 | Batch 000/016 | Cost: 6.8594\n",
      "Epoch: 308/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.24 min\n",
      "Epoch: 309/1000 | Batch 000/016 | Cost: 5.6638\n",
      "Epoch: 309/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.24 min\n",
      "Epoch: 310/1000 | Batch 000/016 | Cost: 7.1888\n",
      "Epoch: 310/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.24 min\n",
      "Epoch: 311/1000 | Batch 000/016 | Cost: 4.8795\n",
      "Epoch: 311/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.24 min\n",
      "Epoch: 312/1000 | Batch 000/016 | Cost: 5.3546\n",
      "Epoch: 312/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.24 min\n",
      "Epoch: 313/1000 | Batch 000/016 | Cost: 6.4520\n",
      "Epoch: 313/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.24 min\n",
      "Epoch: 314/1000 | Batch 000/016 | Cost: 6.7604\n",
      "Epoch: 314/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.24 min\n",
      "Epoch: 315/1000 | Batch 000/016 | Cost: 6.3274\n",
      "Epoch: 315/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.24 min\n",
      "Epoch: 316/1000 | Batch 000/016 | Cost: 6.7715\n",
      "Epoch: 316/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.24 min\n",
      "Epoch: 317/1000 | Batch 000/016 | Cost: 4.8637\n",
      "Epoch: 317/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.24 min\n",
      "Epoch: 318/1000 | Batch 000/016 | Cost: 6.4574\n",
      "Epoch: 318/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.24 min\n",
      "Epoch: 319/1000 | Batch 000/016 | Cost: 5.0649\n",
      "Epoch: 319/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.25 min\n",
      "Epoch: 320/1000 | Batch 000/016 | Cost: 7.4549\n",
      "Epoch: 320/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.25 min\n",
      "Epoch: 321/1000 | Batch 000/016 | Cost: 8.4231\n",
      "Epoch: 321/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.25 min\n",
      "Epoch: 322/1000 | Batch 000/016 | Cost: 6.7356\n",
      "Epoch: 322/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.25 min\n",
      "Epoch: 323/1000 | Batch 000/016 | Cost: 7.4810\n",
      "Epoch: 323/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.25 min\n",
      "Epoch: 324/1000 | Batch 000/016 | Cost: 6.3460\n",
      "Epoch: 324/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.25 min\n",
      "Epoch: 325/1000 | Batch 000/016 | Cost: 7.1769\n",
      "Epoch: 325/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.25 min\n",
      "Epoch: 326/1000 | Batch 000/016 | Cost: 6.2460\n",
      "Epoch: 326/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.25 min\n",
      "Epoch: 327/1000 | Batch 000/016 | Cost: 7.5523\n",
      "Epoch: 327/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.25 min\n",
      "Epoch: 328/1000 | Batch 000/016 | Cost: 6.7856\n",
      "Epoch: 328/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.25 min\n",
      "Epoch: 329/1000 | Batch 000/016 | Cost: 7.7356\n",
      "Epoch: 329/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.25 min\n",
      "Epoch: 330/1000 | Batch 000/016 | Cost: 6.6220\n",
      "Epoch: 330/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.25 min\n",
      "Epoch: 331/1000 | Batch 000/016 | Cost: 6.4935\n",
      "Epoch: 331/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.26 min\n",
      "Epoch: 332/1000 | Batch 000/016 | Cost: 5.7598\n",
      "Epoch: 332/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.26 min\n",
      "Epoch: 333/1000 | Batch 000/016 | Cost: 7.0081\n",
      "Epoch: 333/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.26 min\n",
      "Epoch: 334/1000 | Batch 000/016 | Cost: 7.0416\n",
      "Epoch: 334/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.26 min\n",
      "Epoch: 335/1000 | Batch 000/016 | Cost: 4.9822\n",
      "Epoch: 335/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.26 min\n",
      "Epoch: 336/1000 | Batch 000/016 | Cost: 7.1745\n",
      "Epoch: 336/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.26 min\n",
      "Epoch: 337/1000 | Batch 000/016 | Cost: 7.1833\n",
      "Epoch: 337/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.26 min\n",
      "Epoch: 338/1000 | Batch 000/016 | Cost: 6.1301\n",
      "Epoch: 338/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.26 min\n",
      "Epoch: 339/1000 | Batch 000/016 | Cost: 6.2475\n",
      "Epoch: 339/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.26 min\n",
      "Epoch: 340/1000 | Batch 000/016 | Cost: 5.4771\n",
      "Epoch: 340/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.26 min\n",
      "Epoch: 341/1000 | Batch 000/016 | Cost: 6.4392\n",
      "Epoch: 341/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.26 min\n",
      "Epoch: 342/1000 | Batch 000/016 | Cost: 6.1837\n",
      "Epoch: 342/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.26 min\n",
      "Epoch: 343/1000 | Batch 000/016 | Cost: 6.8884\n",
      "Epoch: 343/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.27 min\n",
      "Epoch: 344/1000 | Batch 000/016 | Cost: 6.1534\n",
      "Epoch: 344/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.27 min\n",
      "Epoch: 345/1000 | Batch 000/016 | Cost: 5.9231\n",
      "Epoch: 345/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.27 min\n",
      "Epoch: 346/1000 | Batch 000/016 | Cost: 6.2893\n",
      "Epoch: 346/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.27 min\n",
      "Epoch: 347/1000 | Batch 000/016 | Cost: 6.4354\n",
      "Epoch: 347/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.27 min\n",
      "Epoch: 348/1000 | Batch 000/016 | Cost: 7.0443\n",
      "Epoch: 348/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.27 min\n",
      "Epoch: 349/1000 | Batch 000/016 | Cost: 5.4421\n",
      "Epoch: 349/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.27 min\n",
      "Epoch: 350/1000 | Batch 000/016 | Cost: 7.1855\n",
      "Epoch: 350/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.27 min\n",
      "Epoch: 351/1000 | Batch 000/016 | Cost: 6.4011\n",
      "Epoch: 351/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.27 min\n",
      "Epoch: 352/1000 | Batch 000/016 | Cost: 6.8978\n",
      "Epoch: 352/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.27 min\n",
      "Epoch: 353/1000 | Batch 000/016 | Cost: 6.9879\n",
      "Epoch: 353/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.27 min\n",
      "Epoch: 354/1000 | Batch 000/016 | Cost: 7.7667\n",
      "Epoch: 354/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.27 min\n",
      "Epoch: 355/1000 | Batch 000/016 | Cost: 5.9554\n",
      "Epoch: 355/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.28 min\n",
      "Epoch: 356/1000 | Batch 000/016 | Cost: 7.2828\n",
      "Epoch: 356/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.28 min\n",
      "Epoch: 357/1000 | Batch 000/016 | Cost: 7.6294\n",
      "Epoch: 357/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.28 min\n",
      "Epoch: 358/1000 | Batch 000/016 | Cost: 6.9693\n",
      "Epoch: 358/1000 Train Loss.: 15.55 | Validation Loss.: 16.41\n",
      "Time elapsed: 0.28 min\n",
      "Epoch: 359/1000 | Batch 000/016 | Cost: 6.1582\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-195-5f0dbeeea5e9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_mse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m         \u001b[0mvalid_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_mse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;31m#         train_acc_lst.append(train_acc)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-195-5f0dbeeea5e9>\u001b[0m in \u001b[0;36mcompute_mse\u001b[0;34m(net, data_loader)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mcurr_mse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_examples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m             \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m             \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    383\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 385\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    386\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontainer_abcs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0mtransposed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdefault_collate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtransposed\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefault_collate_err_msg_format\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontainer_abcs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0mtransposed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdefault_collate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtransposed\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefault_collate_err_msg_format\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m     62\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefault_collate_err_msg_format\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mdefault_collate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# scalars\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mstorage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_new_shared\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0melem_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__module__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'numpy'\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0melem_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'str_'\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0melem_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'string_'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def compute_mse(net, data_loader):\n",
    "    curr_mse, num_examples = torch.zeros(model.num_classes).float(), 0\n",
    "    with torch.no_grad():\n",
    "        for features, targets in data_loader:\n",
    "            features = features.view(-1, num_features).to(DEVICE)\n",
    "            targets = targets.view(-1, 1)\n",
    "            logits, probas = net.forward(features.float())\n",
    "            probas = probas.to(torch.device('cpu'))\n",
    "            loss = torch.sum((targets - probas)**2, dim=0)\n",
    "            num_examples += targets.size(0)\n",
    "            curr_mse += loss\n",
    "\n",
    "        curr_mse = torch.mean(curr_mse/num_examples, dim=0)\n",
    "        return curr_mse\n",
    "\n",
    "def compute_accuracy_and_loss(model, data_loader, device):\n",
    "    correct_pred, num_examples = 0, 0\n",
    "    cross_entropy = 0.\n",
    "    for i, (features, targets) in enumerate(data_loader):\n",
    "            \n",
    "        features = features.view(-1, 28*28).to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        logits, probas = model(features)\n",
    "        cross_entropy += F.cross_entropy(logits, targets).item()\n",
    "        _, predicted_labels = torch.max(probas, 1)\n",
    "        num_examples += targets.size(0)\n",
    "        correct_pred += (predicted_labels == targets).sum()\n",
    "    return correct_pred.float()/num_examples * 100, cross_entropy/num_examples\n",
    "    \n",
    "\n",
    "start_time = time.time()\n",
    "train_acc_lst, valid_acc_lst = [], []\n",
    "train_loss_lst, valid_loss_lst = [], []\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for batch_idx, (features, targets) in enumerate(train_loader):\n",
    "    \n",
    "        ### PREPARE MINIBATCH\n",
    "        features = features.view(-1, num_features).to(DEVICE)\n",
    "        targets = targets.view(-1, 1).to(DEVICE)\n",
    "        \n",
    "        ### FORWARD AND BACK PROP\n",
    "        logits, probas = model(features.float())\n",
    "#         print(\"Logits\")\n",
    "#         print(logits)\n",
    "#         print(\"Probas\")\n",
    "#         print(probas)\n",
    "        cost = F.mse_loss(targets, logits)\n",
    "#         print(cost)\n",
    "        optimizer.zero_grad()\n",
    "        cost.backward()\n",
    "        \n",
    "        ### UPDATE MODEL PARAMETERS\n",
    "        optimizer.step()\n",
    "        \n",
    "        ### LOGGING\n",
    "        if not batch_idx % 200:\n",
    "            print (f'Epoch: {epoch+1:03d}/{NUM_EPOCHS:03d} | '\n",
    "                   f'Batch {batch_idx:03d}/{len(train_loader):03d} |' \n",
    "                   f' Cost: {cost:.4f}')\n",
    "\n",
    "    # no need to build the computation graph for backprop when computing accuracy\n",
    "    model.eval()\n",
    "    with torch.set_grad_enabled(False):\n",
    "        train_loss = compute_mse(model, train_loader)\n",
    "        valid_loss = compute_mse(model, valid_loader)\n",
    "#         train_acc_lst.append(train_acc)\n",
    "#         valid_acc_lst.append(valid_acc)\n",
    "        train_loss_lst.append(train_loss)\n",
    "        valid_loss_lst.append(valid_loss)\n",
    "        print(f'Epoch: {epoch+1:03d}/{NUM_EPOCHS:03d} Train Loss.: {train_loss:.2f}'\n",
    "              f' | Validation Loss.: {valid_loss:.2f}')\n",
    "        \n",
    "    elapsed = (time.time() - start_time)/60\n",
    "    print(f'Time elapsed: {elapsed:.2f} min')\n",
    "  \n",
    "elapsed = (time.time() - start_time)/60\n",
    "print(f'Total Training Time: {elapsed:.2f} min')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation (No Need To Change Any Code in This Section!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(range(1, NUM_EPOCHS+1), train_loss_lst, label='Training loss')\n",
    "plt.plot(range(1, NUM_EPOCHS+1), valid_loss_lst, label='Validation loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.ylabel('Cross entropy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "x and y must have same first dimension, but have shapes (1000,) and (0,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-166-5d12949236de>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNUM_EPOCHS\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc_lst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Training accuracy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNUM_EPOCHS\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_acc_lst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Validation accuracy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'upper left'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Cross entropy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Epoch'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2793\u001b[0m     return gca().plot(\n\u001b[1;32m   2794\u001b[0m         *args, scalex=scalex, scaley=scaley, **({\"data\": data} if data\n\u001b[0;32m-> 2795\u001b[0;31m         is not None else {}), **kwargs)\n\u001b[0m\u001b[1;32m   2796\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2797\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1664\u001b[0m         \"\"\"\n\u001b[1;32m   1665\u001b[0m         \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmlines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLine2D\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_alias_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1666\u001b[0;31m         \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1667\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1668\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    223\u001b[0m                 \u001b[0mthis\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 225\u001b[0;31m             \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_plot_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_next_color\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m_plot_args\u001b[0;34m(self, tup, kwargs)\u001b[0m\n\u001b[1;32m    389\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindex_of\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 391\u001b[0;31m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_xy_from_xy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    392\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommand\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'plot'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m_xy_from_xy\u001b[0;34m(self, x, y)\u001b[0m\n\u001b[1;32m    268\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m             raise ValueError(\"x and y must have same first dimension, but \"\n\u001b[0;32m--> 270\u001b[0;31m                              \"have shapes {} and {}\".format(x.shape, y.shape))\n\u001b[0m\u001b[1;32m    271\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m             raise ValueError(\"x and y can be no greater than 2-D, but have \"\n",
      "\u001b[0;31mValueError\u001b[0m: x and y must have same first dimension, but have shapes (1000,) and (0,)"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(range(1, NUM_EPOCHS+1), train_acc_lst, label='Training accuracy')\n",
    "plt.plot(range(1, NUM_EPOCHS+1), valid_acc_lst, label='Validation accuracy')\n",
    "plt.legend(loc='upper left')\n",
    "plt.ylabel('Cross entropy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "shape '[-1, 784]' is invalid for input of size 9152",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-167-400cb4f49555>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# save memory during inference\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mtest_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_accuracy_and_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Test accuracy: {test_acc:.2f}%'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-163-5f0dbeeea5e9>\u001b[0m in \u001b[0;36mcompute_accuracy_and_loss\u001b[0;34m(model, data_loader, device)\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m28\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[-1, 784]' is invalid for input of size 9152"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.set_grad_enabled(False): # save memory during inference\n",
    "    test_acc, test_loss = compute_accuracy_and_loss(model, test_loader, DEVICE)\n",
    "    print(f'Test accuracy: {test_acc:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3.5908, 4.4296, 3.9549, 3.7788, 3.4959, 4.5799, 4.3617, 3.6985, 3.8059,\n",
      "         3.8352, 3.9735, 3.8603, 4.0253, 3.6682, 3.7194, 4.0383, 4.3566, 4.0029,\n",
      "         3.7414, 4.1916, 3.9318, 3.9776, 4.5796, 3.9426, 3.9024, 3.7849, 3.5381,\n",
      "         4.2494, 3.8268, 3.9785, 3.6572, 4.1440, 3.9138, 3.9181, 3.9783, 4.3566,\n",
      "         4.0373, 3.7403, 3.9140, 3.7997, 3.9013, 3.6901, 3.7277, 3.7763, 3.6799,\n",
      "         3.7964, 3.6726, 4.0521, 3.8022, 3.9083, 4.1519, 4.0526, 3.5831, 3.8200,\n",
      "         4.4837, 4.1841, 4.5327, 4.2134, 3.9183, 3.8317, 4.0665, 4.1985, 4.1133,\n",
      "         3.4241]])\n",
      "tensor([0., 8., 1., 4., 1., 2., 0., 7., 3., 0., 5., 1., 3., 0., 0., 3., 2., 4.,\n",
      "        8., 1., 7., 6., 1., 1., 5., 8., 1., 8., 5., 6., 7., 2., 1., 0., 2., 7.,\n",
      "        3., 0., 6., 6., 2., 4., 0., 8., 0., 7., 0., 1., 2., 2., 4., 4., 3., 2.,\n",
      "        4., 7., 0., 4., 8., 8., 1., 4., 5., 2.])\n"
     ]
    }
   ],
   "source": [
    "num = 0\n",
    "\n",
    "model.eval()\n",
    "with torch.set_grad_enabled(False): # save memory during inference\n",
    "    for i, (x, y) in enumerate(test_loader):\n",
    "        if i == num:\n",
    "            logits, probas = model(x)\n",
    "            pred = np.argmax(probas, axis=1)\n",
    "            print(logits.view(1,-1))\n",
    "            print(y)\n",
    "#             print(y[4].item() == np.argmax(probas, axis=1)[4].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
