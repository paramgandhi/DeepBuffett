{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import random\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 123\n",
    "DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StockDataset(Dataset):\n",
    "    def __init__(self, csv_path):\n",
    "        df = pd.read_csv(csv_path)\n",
    "        self.y = df['Label'].to_numpy().astype(int)\n",
    "        self.features = df.drop([\"Company\", \"Date\", \"Target\", \"Label\"], axis=1).to_numpy().astype('float32')\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        features = self.features[index]\n",
    "        label = self.y[index]\n",
    "        \n",
    "        return features, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.y.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = StockDataset(csv_path='train_normalized.csv')\n",
    "\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset,\n",
    "                          batch_size=BATCH_SIZE,\n",
    "                          shuffle=True,\n",
    "                          num_workers=4)\n",
    "\n",
    "\n",
    "valid_dataset = StockDataset(csv_path='val_normalized.csv')\n",
    "\n",
    "valid_loader = DataLoader(dataset=valid_dataset,\n",
    "                          batch_size=BATCH_SIZE,\n",
    "                          shuffle=False,\n",
    "                          num_workers=4)\n",
    "\n",
    "test_dataset = StockDataset(csv_path='test_normalized.csv')\n",
    "\n",
    "test_loader = DataLoader(dataset=test_dataset,\n",
    "                         batch_size=BATCH_SIZE,\n",
    "                         shuffle=False,\n",
    "                         num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = 143\n",
    "num_classes = 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | Batch index: 0 | Batch size: 64\n",
      "break minibatch for-loop\n",
      "Epoch: 2 | Batch index: 0 | Batch size: 64\n",
      "break minibatch for-loop\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "num_epochs = 2\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    for batch_idx, (x, y) in enumerate(train_loader):\n",
    "        \n",
    "        print('Epoch:', epoch+1, end='')\n",
    "        print(' | Batch index:', batch_idx, end='')\n",
    "        print(' | Batch size:', y.size()[0])\n",
    "        num_features = x.shape[1]\n",
    "        x = x.to(DEVICE)\n",
    "        y = y.to(DEVICE)\n",
    "        print('break minibatch for-loop')\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxRegression(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, num_features, num_classes):\n",
    "        super(SoftmaxRegression, self).__init__()\n",
    "        self.linear = torch.nn.Linear(num_features, num_classes)\n",
    "        \n",
    "        self.linear.weight.detach().zero_()\n",
    "        self.linear.bias.detach().zero_()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        logits = self.linear(x)\n",
    "        probas = F.softmax(logits, dim=1)\n",
    "        return logits, probas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x10700b890>"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SoftmaxRegression(num_features=143, num_classes=9)\n",
    "model = model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001/050 | Batch 000/977 | Cost: 2.1972\n",
      "Epoch: 001/050 | Batch 200/977 | Cost: 2.1945\n",
      "Epoch: 001/050 | Batch 400/977 | Cost: 2.1553\n",
      "Epoch: 001/050 | Batch 600/977 | Cost: 2.1926\n",
      "Epoch: 001/050 | Batch 800/977 | Cost: 2.2026\n",
      "Epoch: 001/050 training accuracy: 15.32%\n",
      "Time elapsed: 0.02 min\n",
      "Epoch: 002/050 | Batch 000/977 | Cost: 2.1696\n",
      "Epoch: 002/050 | Batch 200/977 | Cost: 2.1848\n",
      "Epoch: 002/050 | Batch 400/977 | Cost: 2.2144\n",
      "Epoch: 002/050 | Batch 600/977 | Cost: 2.2548\n",
      "Epoch: 002/050 | Batch 800/977 | Cost: 2.2000\n",
      "Epoch: 002/050 training accuracy: 16.55%\n",
      "Time elapsed: 0.05 min\n",
      "Epoch: 003/050 | Batch 000/977 | Cost: 2.1715\n",
      "Epoch: 003/050 | Batch 200/977 | Cost: 2.2162\n",
      "Epoch: 003/050 | Batch 400/977 | Cost: 2.1391\n",
      "Epoch: 003/050 | Batch 600/977 | Cost: 2.2243\n",
      "Epoch: 003/050 | Batch 800/977 | Cost: 2.2150\n",
      "Epoch: 003/050 training accuracy: 15.43%\n",
      "Time elapsed: 0.07 min\n",
      "Epoch: 004/050 | Batch 000/977 | Cost: 2.2312\n",
      "Epoch: 004/050 | Batch 200/977 | Cost: 2.2149\n",
      "Epoch: 004/050 | Batch 400/977 | Cost: 2.1645\n",
      "Epoch: 004/050 | Batch 600/977 | Cost: 2.2068\n",
      "Epoch: 004/050 | Batch 800/977 | Cost: 2.2076\n",
      "Epoch: 004/050 training accuracy: 17.36%\n",
      "Time elapsed: 0.09 min\n",
      "Epoch: 005/050 | Batch 000/977 | Cost: 2.1878\n",
      "Epoch: 005/050 | Batch 200/977 | Cost: 2.2162\n",
      "Epoch: 005/050 | Batch 400/977 | Cost: 2.1808\n",
      "Epoch: 005/050 | Batch 600/977 | Cost: 2.1386\n",
      "Epoch: 005/050 | Batch 800/977 | Cost: 2.1353\n",
      "Epoch: 005/050 training accuracy: 16.35%\n",
      "Time elapsed: 0.12 min\n",
      "Epoch: 006/050 | Batch 000/977 | Cost: 2.2583\n",
      "Epoch: 006/050 | Batch 200/977 | Cost: 2.1426\n",
      "Epoch: 006/050 | Batch 400/977 | Cost: 2.0815\n",
      "Epoch: 006/050 | Batch 600/977 | Cost: 2.2066\n",
      "Epoch: 006/050 | Batch 800/977 | Cost: 2.2411\n",
      "Epoch: 006/050 training accuracy: 15.29%\n",
      "Time elapsed: 0.14 min\n",
      "Epoch: 007/050 | Batch 000/977 | Cost: 2.1895\n",
      "Epoch: 007/050 | Batch 200/977 | Cost: 2.2052\n",
      "Epoch: 007/050 | Batch 400/977 | Cost: 2.2025\n",
      "Epoch: 007/050 | Batch 600/977 | Cost: 2.2373\n",
      "Epoch: 007/050 | Batch 800/977 | Cost: 2.2105\n",
      "Epoch: 007/050 training accuracy: 16.88%\n",
      "Time elapsed: 0.17 min\n",
      "Epoch: 008/050 | Batch 000/977 | Cost: 2.1738\n",
      "Epoch: 008/050 | Batch 200/977 | Cost: 2.1936\n",
      "Epoch: 008/050 | Batch 400/977 | Cost: 2.1606\n",
      "Epoch: 008/050 | Batch 600/977 | Cost: 2.1647\n",
      "Epoch: 008/050 | Batch 800/977 | Cost: 2.2107\n",
      "Epoch: 008/050 training accuracy: 16.77%\n",
      "Time elapsed: 0.19 min\n",
      "Epoch: 009/050 | Batch 000/977 | Cost: 2.1677\n",
      "Epoch: 009/050 | Batch 200/977 | Cost: 2.2255\n",
      "Epoch: 009/050 | Batch 400/977 | Cost: 2.1769\n",
      "Epoch: 009/050 | Batch 600/977 | Cost: 2.2017\n",
      "Epoch: 009/050 | Batch 800/977 | Cost: 2.1214\n",
      "Epoch: 009/050 training accuracy: 17.49%\n",
      "Time elapsed: 0.22 min\n",
      "Epoch: 010/050 | Batch 000/977 | Cost: 2.1296\n",
      "Epoch: 010/050 | Batch 200/977 | Cost: 2.2092\n",
      "Epoch: 010/050 | Batch 400/977 | Cost: 2.1452\n",
      "Epoch: 010/050 | Batch 600/977 | Cost: 2.1893\n",
      "Epoch: 010/050 | Batch 800/977 | Cost: 2.1607\n",
      "Epoch: 010/050 training accuracy: 16.82%\n",
      "Time elapsed: 0.24 min\n",
      "Epoch: 011/050 | Batch 000/977 | Cost: 2.2274\n",
      "Epoch: 011/050 | Batch 200/977 | Cost: 2.1849\n",
      "Epoch: 011/050 | Batch 400/977 | Cost: 2.2063\n",
      "Epoch: 011/050 | Batch 600/977 | Cost: 2.1742\n",
      "Epoch: 011/050 | Batch 800/977 | Cost: 2.1579\n",
      "Epoch: 011/050 training accuracy: 17.19%\n",
      "Time elapsed: 0.26 min\n",
      "Epoch: 012/050 | Batch 000/977 | Cost: 2.2128\n",
      "Epoch: 012/050 | Batch 200/977 | Cost: 2.1531\n",
      "Epoch: 012/050 | Batch 400/977 | Cost: 2.2169\n",
      "Epoch: 012/050 | Batch 600/977 | Cost: 2.1684\n",
      "Epoch: 012/050 | Batch 800/977 | Cost: 2.1751\n",
      "Epoch: 012/050 training accuracy: 17.46%\n",
      "Time elapsed: 0.29 min\n",
      "Epoch: 013/050 | Batch 000/977 | Cost: 2.1148\n",
      "Epoch: 013/050 | Batch 200/977 | Cost: 2.2137\n",
      "Epoch: 013/050 | Batch 400/977 | Cost: 2.2106\n",
      "Epoch: 013/050 | Batch 600/977 | Cost: 2.1544\n",
      "Epoch: 013/050 | Batch 800/977 | Cost: 2.1973\n",
      "Epoch: 013/050 training accuracy: 16.88%\n",
      "Time elapsed: 0.31 min\n",
      "Epoch: 014/050 | Batch 000/977 | Cost: 2.0822\n",
      "Epoch: 014/050 | Batch 200/977 | Cost: 2.1788\n",
      "Epoch: 014/050 | Batch 400/977 | Cost: 2.1746\n",
      "Epoch: 014/050 | Batch 600/977 | Cost: 2.2325\n",
      "Epoch: 014/050 | Batch 800/977 | Cost: 2.2871\n",
      "Epoch: 014/050 training accuracy: 17.40%\n",
      "Time elapsed: 0.34 min\n",
      "Epoch: 015/050 | Batch 000/977 | Cost: 2.1534\n",
      "Epoch: 015/050 | Batch 200/977 | Cost: 2.2085\n",
      "Epoch: 015/050 | Batch 400/977 | Cost: 2.2250\n",
      "Epoch: 015/050 | Batch 600/977 | Cost: 2.2261\n",
      "Epoch: 015/050 | Batch 800/977 | Cost: 2.1686\n",
      "Epoch: 015/050 training accuracy: 17.70%\n",
      "Time elapsed: 0.36 min\n",
      "Epoch: 016/050 | Batch 000/977 | Cost: 2.1830\n",
      "Epoch: 016/050 | Batch 200/977 | Cost: 2.1346\n",
      "Epoch: 016/050 | Batch 400/977 | Cost: 2.1876\n",
      "Epoch: 016/050 | Batch 600/977 | Cost: 2.1388\n",
      "Epoch: 016/050 | Batch 800/977 | Cost: 2.1740\n",
      "Epoch: 016/050 training accuracy: 17.96%\n",
      "Time elapsed: 0.38 min\n",
      "Epoch: 017/050 | Batch 000/977 | Cost: 2.2055\n",
      "Epoch: 017/050 | Batch 200/977 | Cost: 2.2220\n",
      "Epoch: 017/050 | Batch 400/977 | Cost: 2.2072\n",
      "Epoch: 017/050 | Batch 600/977 | Cost: 2.1920\n",
      "Epoch: 017/050 | Batch 800/977 | Cost: 2.1623\n",
      "Epoch: 017/050 training accuracy: 17.50%\n",
      "Time elapsed: 0.41 min\n",
      "Epoch: 018/050 | Batch 000/977 | Cost: 2.2167\n",
      "Epoch: 018/050 | Batch 200/977 | Cost: 2.1742\n",
      "Epoch: 018/050 | Batch 400/977 | Cost: 2.2013\n",
      "Epoch: 018/050 | Batch 600/977 | Cost: 2.1354\n",
      "Epoch: 018/050 | Batch 800/977 | Cost: 2.1548\n",
      "Epoch: 018/050 training accuracy: 17.30%\n",
      "Time elapsed: 0.43 min\n",
      "Epoch: 019/050 | Batch 000/977 | Cost: 2.1982\n",
      "Epoch: 019/050 | Batch 200/977 | Cost: 2.1963\n",
      "Epoch: 019/050 | Batch 400/977 | Cost: 2.1893\n",
      "Epoch: 019/050 | Batch 600/977 | Cost: 2.1540\n",
      "Epoch: 019/050 | Batch 800/977 | Cost: 2.0873\n",
      "Epoch: 019/050 training accuracy: 17.43%\n",
      "Time elapsed: 0.46 min\n",
      "Epoch: 020/050 | Batch 000/977 | Cost: 2.1867\n",
      "Epoch: 020/050 | Batch 200/977 | Cost: 2.1539\n",
      "Epoch: 020/050 | Batch 400/977 | Cost: 2.1622\n",
      "Epoch: 020/050 | Batch 600/977 | Cost: 2.1834\n",
      "Epoch: 020/050 | Batch 800/977 | Cost: 2.2528\n",
      "Epoch: 020/050 training accuracy: 18.27%\n",
      "Time elapsed: 0.48 min\n",
      "Epoch: 021/050 | Batch 000/977 | Cost: 2.2240\n",
      "Epoch: 021/050 | Batch 200/977 | Cost: 2.2328\n",
      "Epoch: 021/050 | Batch 400/977 | Cost: 2.1497\n",
      "Epoch: 021/050 | Batch 600/977 | Cost: 2.1958\n",
      "Epoch: 021/050 | Batch 800/977 | Cost: 2.1273\n",
      "Epoch: 021/050 training accuracy: 16.98%\n",
      "Time elapsed: 0.51 min\n",
      "Epoch: 022/050 | Batch 000/977 | Cost: 2.1984\n",
      "Epoch: 022/050 | Batch 200/977 | Cost: 2.2237\n",
      "Epoch: 022/050 | Batch 400/977 | Cost: 2.2406\n",
      "Epoch: 022/050 | Batch 600/977 | Cost: 2.2153\n",
      "Epoch: 022/050 | Batch 800/977 | Cost: 2.1649\n",
      "Epoch: 022/050 training accuracy: 18.05%\n",
      "Time elapsed: 0.53 min\n",
      "Epoch: 023/050 | Batch 000/977 | Cost: 2.2036\n",
      "Epoch: 023/050 | Batch 200/977 | Cost: 2.1958\n",
      "Epoch: 023/050 | Batch 400/977 | Cost: 2.2259\n",
      "Epoch: 023/050 | Batch 600/977 | Cost: 2.1630\n",
      "Epoch: 023/050 | Batch 800/977 | Cost: 2.1040\n",
      "Epoch: 023/050 training accuracy: 16.80%\n",
      "Time elapsed: 0.56 min\n",
      "Epoch: 024/050 | Batch 000/977 | Cost: 2.2291\n",
      "Epoch: 024/050 | Batch 200/977 | Cost: 2.1904\n",
      "Epoch: 024/050 | Batch 400/977 | Cost: 2.1152\n",
      "Epoch: 024/050 | Batch 600/977 | Cost: 2.1416\n",
      "Epoch: 024/050 | Batch 800/977 | Cost: 2.1586\n",
      "Epoch: 024/050 training accuracy: 17.41%\n",
      "Time elapsed: 0.58 min\n",
      "Epoch: 025/050 | Batch 000/977 | Cost: 2.1929\n",
      "Epoch: 025/050 | Batch 200/977 | Cost: 2.2540\n",
      "Epoch: 025/050 | Batch 400/977 | Cost: 2.1451\n",
      "Epoch: 025/050 | Batch 600/977 | Cost: 2.1854\n",
      "Epoch: 025/050 | Batch 800/977 | Cost: 2.1971\n",
      "Epoch: 025/050 training accuracy: 17.87%\n",
      "Time elapsed: 0.61 min\n",
      "Epoch: 026/050 | Batch 000/977 | Cost: 2.1786\n",
      "Epoch: 026/050 | Batch 200/977 | Cost: 2.1806\n",
      "Epoch: 026/050 | Batch 400/977 | Cost: 2.1503\n",
      "Epoch: 026/050 | Batch 600/977 | Cost: 2.1389\n",
      "Epoch: 026/050 | Batch 800/977 | Cost: 2.1201\n",
      "Epoch: 026/050 training accuracy: 17.88%\n",
      "Time elapsed: 0.63 min\n",
      "Epoch: 027/050 | Batch 000/977 | Cost: 2.1898\n",
      "Epoch: 027/050 | Batch 200/977 | Cost: 2.1193\n",
      "Epoch: 027/050 | Batch 400/977 | Cost: 2.1412\n",
      "Epoch: 027/050 | Batch 600/977 | Cost: 2.2236\n",
      "Epoch: 027/050 | Batch 800/977 | Cost: 2.1802\n",
      "Epoch: 027/050 training accuracy: 18.42%\n",
      "Time elapsed: 0.66 min\n",
      "Epoch: 028/050 | Batch 000/977 | Cost: 2.1400\n",
      "Epoch: 028/050 | Batch 200/977 | Cost: 2.1837\n",
      "Epoch: 028/050 | Batch 400/977 | Cost: 2.1851\n",
      "Epoch: 028/050 | Batch 600/977 | Cost: 2.1154\n",
      "Epoch: 028/050 | Batch 800/977 | Cost: 2.1676\n",
      "Epoch: 028/050 training accuracy: 18.62%\n",
      "Time elapsed: 0.68 min\n",
      "Epoch: 029/050 | Batch 000/977 | Cost: 2.1459\n",
      "Epoch: 029/050 | Batch 200/977 | Cost: 2.1195\n",
      "Epoch: 029/050 | Batch 400/977 | Cost: 2.1475\n",
      "Epoch: 029/050 | Batch 600/977 | Cost: 2.2583\n",
      "Epoch: 029/050 | Batch 800/977 | Cost: 2.1755\n",
      "Epoch: 029/050 training accuracy: 17.77%\n",
      "Time elapsed: 0.71 min\n",
      "Epoch: 030/050 | Batch 000/977 | Cost: 2.0770\n",
      "Epoch: 030/050 | Batch 200/977 | Cost: 2.1168\n",
      "Epoch: 030/050 | Batch 400/977 | Cost: 2.1844\n",
      "Epoch: 030/050 | Batch 600/977 | Cost: 2.1596\n",
      "Epoch: 030/050 | Batch 800/977 | Cost: 2.1705\n",
      "Epoch: 030/050 training accuracy: 17.50%\n",
      "Time elapsed: 0.73 min\n",
      "Epoch: 031/050 | Batch 000/977 | Cost: 2.0879\n",
      "Epoch: 031/050 | Batch 200/977 | Cost: 2.1595\n",
      "Epoch: 031/050 | Batch 400/977 | Cost: 2.1149\n",
      "Epoch: 031/050 | Batch 600/977 | Cost: 2.1298\n",
      "Epoch: 031/050 | Batch 800/977 | Cost: 2.2433\n",
      "Epoch: 031/050 training accuracy: 18.66%\n",
      "Time elapsed: 0.76 min\n",
      "Epoch: 032/050 | Batch 000/977 | Cost: 2.1603\n",
      "Epoch: 032/050 | Batch 200/977 | Cost: 2.2103\n",
      "Epoch: 032/050 | Batch 400/977 | Cost: 2.2200\n",
      "Epoch: 032/050 | Batch 600/977 | Cost: 2.1703\n",
      "Epoch: 032/050 | Batch 800/977 | Cost: 2.1901\n",
      "Epoch: 032/050 training accuracy: 18.86%\n",
      "Time elapsed: 0.78 min\n",
      "Epoch: 033/050 | Batch 000/977 | Cost: 2.2262\n",
      "Epoch: 033/050 | Batch 200/977 | Cost: 2.2159\n",
      "Epoch: 033/050 | Batch 400/977 | Cost: 2.2138\n",
      "Epoch: 033/050 | Batch 600/977 | Cost: 2.1446\n",
      "Epoch: 033/050 | Batch 800/977 | Cost: 2.2231\n",
      "Epoch: 033/050 training accuracy: 18.32%\n",
      "Time elapsed: 0.81 min\n",
      "Epoch: 034/050 | Batch 000/977 | Cost: 2.2170\n",
      "Epoch: 034/050 | Batch 200/977 | Cost: 2.1246\n",
      "Epoch: 034/050 | Batch 400/977 | Cost: 2.2269\n",
      "Epoch: 034/050 | Batch 600/977 | Cost: 2.1837\n",
      "Epoch: 034/050 | Batch 800/977 | Cost: 2.1925\n",
      "Epoch: 034/050 training accuracy: 18.38%\n",
      "Time elapsed: 0.83 min\n",
      "Epoch: 035/050 | Batch 000/977 | Cost: 2.0775\n",
      "Epoch: 035/050 | Batch 200/977 | Cost: 2.1991\n",
      "Epoch: 035/050 | Batch 400/977 | Cost: 2.1978\n",
      "Epoch: 035/050 | Batch 600/977 | Cost: 2.1961\n",
      "Epoch: 035/050 | Batch 800/977 | Cost: 2.1375\n",
      "Epoch: 035/050 training accuracy: 18.33%\n",
      "Time elapsed: 0.86 min\n",
      "Epoch: 036/050 | Batch 000/977 | Cost: 2.1984\n",
      "Epoch: 036/050 | Batch 200/977 | Cost: 2.1757\n",
      "Epoch: 036/050 | Batch 400/977 | Cost: 2.1810\n",
      "Epoch: 036/050 | Batch 600/977 | Cost: 2.2281\n",
      "Epoch: 036/050 | Batch 800/977 | Cost: 2.1559\n",
      "Epoch: 036/050 training accuracy: 17.66%\n",
      "Time elapsed: 0.88 min\n",
      "Epoch: 037/050 | Batch 000/977 | Cost: 2.1572\n",
      "Epoch: 037/050 | Batch 200/977 | Cost: 2.1283\n",
      "Epoch: 037/050 | Batch 400/977 | Cost: 2.2244\n",
      "Epoch: 037/050 | Batch 600/977 | Cost: 2.1607\n",
      "Epoch: 037/050 | Batch 800/977 | Cost: 2.1868\n",
      "Epoch: 037/050 training accuracy: 18.32%\n",
      "Time elapsed: 0.91 min\n",
      "Epoch: 038/050 | Batch 000/977 | Cost: 2.1648\n",
      "Epoch: 038/050 | Batch 200/977 | Cost: 2.2526\n",
      "Epoch: 038/050 | Batch 400/977 | Cost: 2.2514\n",
      "Epoch: 038/050 | Batch 600/977 | Cost: 2.2039\n",
      "Epoch: 038/050 | Batch 800/977 | Cost: 2.1455\n",
      "Epoch: 038/050 training accuracy: 17.91%\n",
      "Time elapsed: 0.93 min\n",
      "Epoch: 039/050 | Batch 000/977 | Cost: 2.0867\n",
      "Epoch: 039/050 | Batch 200/977 | Cost: 2.1228\n",
      "Epoch: 039/050 | Batch 400/977 | Cost: 2.2168\n",
      "Epoch: 039/050 | Batch 600/977 | Cost: 2.1407\n",
      "Epoch: 039/050 | Batch 800/977 | Cost: 2.2409\n",
      "Epoch: 039/050 training accuracy: 18.75%\n",
      "Time elapsed: 0.96 min\n",
      "Epoch: 040/050 | Batch 000/977 | Cost: 2.2011\n",
      "Epoch: 040/050 | Batch 200/977 | Cost: 2.1861\n",
      "Epoch: 040/050 | Batch 400/977 | Cost: 2.1762\n",
      "Epoch: 040/050 | Batch 600/977 | Cost: 2.1603\n",
      "Epoch: 040/050 | Batch 800/977 | Cost: 2.2044\n",
      "Epoch: 040/050 training accuracy: 18.14%\n",
      "Time elapsed: 0.98 min\n",
      "Epoch: 041/050 | Batch 000/977 | Cost: 2.1608\n",
      "Epoch: 041/050 | Batch 200/977 | Cost: 2.1931\n",
      "Epoch: 041/050 | Batch 400/977 | Cost: 2.1904\n",
      "Epoch: 041/050 | Batch 600/977 | Cost: 2.1350\n",
      "Epoch: 041/050 | Batch 800/977 | Cost: 2.2040\n",
      "Epoch: 041/050 training accuracy: 18.66%\n",
      "Time elapsed: 1.01 min\n",
      "Epoch: 042/050 | Batch 000/977 | Cost: 2.1030\n",
      "Epoch: 042/050 | Batch 200/977 | Cost: 2.2001\n",
      "Epoch: 042/050 | Batch 400/977 | Cost: 2.2290\n",
      "Epoch: 042/050 | Batch 600/977 | Cost: 2.1840\n",
      "Epoch: 042/050 | Batch 800/977 | Cost: 2.1620\n",
      "Epoch: 042/050 training accuracy: 17.86%\n",
      "Time elapsed: 1.03 min\n",
      "Epoch: 043/050 | Batch 000/977 | Cost: 2.1694\n",
      "Epoch: 043/050 | Batch 200/977 | Cost: 2.1730\n",
      "Epoch: 043/050 | Batch 400/977 | Cost: 2.1626\n",
      "Epoch: 043/050 | Batch 600/977 | Cost: 2.2477\n",
      "Epoch: 043/050 | Batch 800/977 | Cost: 2.2146\n",
      "Epoch: 043/050 training accuracy: 18.21%\n",
      "Time elapsed: 1.06 min\n",
      "Epoch: 044/050 | Batch 000/977 | Cost: 2.2352\n",
      "Epoch: 044/050 | Batch 200/977 | Cost: 2.2073\n",
      "Epoch: 044/050 | Batch 400/977 | Cost: 2.1910\n",
      "Epoch: 044/050 | Batch 600/977 | Cost: 2.2258\n",
      "Epoch: 044/050 | Batch 800/977 | Cost: 2.1164\n",
      "Epoch: 044/050 training accuracy: 18.66%\n",
      "Time elapsed: 1.08 min\n",
      "Epoch: 045/050 | Batch 000/977 | Cost: 2.1432\n",
      "Epoch: 045/050 | Batch 200/977 | Cost: 2.1639\n",
      "Epoch: 045/050 | Batch 400/977 | Cost: 2.1376\n",
      "Epoch: 045/050 | Batch 600/977 | Cost: 2.1687\n",
      "Epoch: 045/050 | Batch 800/977 | Cost: 2.1809\n",
      "Epoch: 045/050 training accuracy: 18.78%\n",
      "Time elapsed: 1.11 min\n",
      "Epoch: 046/050 | Batch 000/977 | Cost: 2.2002\n",
      "Epoch: 046/050 | Batch 200/977 | Cost: 2.2373\n",
      "Epoch: 046/050 | Batch 400/977 | Cost: 2.2732\n",
      "Epoch: 046/050 | Batch 600/977 | Cost: 2.2105\n",
      "Epoch: 046/050 | Batch 800/977 | Cost: 2.1813\n",
      "Epoch: 046/050 training accuracy: 18.08%\n",
      "Time elapsed: 1.13 min\n",
      "Epoch: 047/050 | Batch 000/977 | Cost: 2.2045\n",
      "Epoch: 047/050 | Batch 200/977 | Cost: 2.1447\n",
      "Epoch: 047/050 | Batch 400/977 | Cost: 2.2616\n",
      "Epoch: 047/050 | Batch 600/977 | Cost: 2.1956\n",
      "Epoch: 047/050 | Batch 800/977 | Cost: 2.1992\n",
      "Epoch: 047/050 training accuracy: 17.86%\n",
      "Time elapsed: 1.16 min\n",
      "Epoch: 048/050 | Batch 000/977 | Cost: 2.2008\n",
      "Epoch: 048/050 | Batch 200/977 | Cost: 2.1430\n",
      "Epoch: 048/050 | Batch 400/977 | Cost: 2.2274\n",
      "Epoch: 048/050 | Batch 600/977 | Cost: 2.1636\n",
      "Epoch: 048/050 | Batch 800/977 | Cost: 2.1989\n",
      "Epoch: 048/050 training accuracy: 18.53%\n",
      "Time elapsed: 1.18 min\n",
      "Epoch: 049/050 | Batch 000/977 | Cost: 2.1864\n",
      "Epoch: 049/050 | Batch 200/977 | Cost: 2.2156\n",
      "Epoch: 049/050 | Batch 400/977 | Cost: 2.2153\n",
      "Epoch: 049/050 | Batch 600/977 | Cost: 2.2244\n",
      "Epoch: 049/050 | Batch 800/977 | Cost: 2.2313\n",
      "Epoch: 049/050 training accuracy: 18.58%\n",
      "Time elapsed: 1.20 min\n",
      "Epoch: 050/050 | Batch 000/977 | Cost: 2.2244\n",
      "Epoch: 050/050 | Batch 200/977 | Cost: 2.2032\n",
      "Epoch: 050/050 | Batch 400/977 | Cost: 2.1379\n",
      "Epoch: 050/050 | Batch 600/977 | Cost: 2.1667\n",
      "Epoch: 050/050 | Batch 800/977 | Cost: 2.1423\n",
      "Epoch: 050/050 training accuracy: 17.85%\n",
      "Time elapsed: 1.23 min\n"
     ]
    }
   ],
   "source": [
    "def compute_accuracy(model, data_loader):\n",
    "    correct_pred, num_examples = 0, 0\n",
    "    \n",
    "    for features, targets in data_loader:\n",
    "        features = features.view(-1, num_features).to(DEVICE)\n",
    "        targets = targets.to(DEVICE)\n",
    "        logits, probas = model(features)\n",
    "        _, predicted_labels = torch.max(probas, 1)\n",
    "        num_examples += targets.size(0)\n",
    "        correct_pred += (predicted_labels == targets).sum()\n",
    "        \n",
    "    return correct_pred.float() / num_examples * 100\n",
    "max_runs = 2\n",
    "runs = 0\n",
    "start_time = time.time()\n",
    "epoch_costs = []\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    avg_cost = 0.\n",
    "    for batch_idx, (features, targets) in enumerate(train_loader):\n",
    "        runs = runs+1\n",
    "        features = features.view(-1, num_features).to(DEVICE)\n",
    "        targets = targets.to(DEVICE)\n",
    "            \n",
    "        ### FORWARD AND BACK PROP\n",
    "        logits, probas = model(features)\n",
    "        \n",
    "        # note that the PyTorch implementation of\n",
    "        # CrossEntropyLoss works with logits, not\n",
    "        # probabilities\n",
    "        cost = F.cross_entropy(probas, targets)\n",
    "        optimizer.zero_grad()\n",
    "        cost.backward()\n",
    "        avg_cost += cost\n",
    "        avg_cost\n",
    "        \n",
    "#         print(\"Logits:\")\n",
    "#         display(logits)\n",
    "#         print(\"Probas:\")\n",
    "#         display(probas)\n",
    "#         print(\"Targets:\")\n",
    "#         display(targets)\n",
    "#         print(\"Cost:\")\n",
    "#         display(cost)\n",
    "#         print()\n",
    "#         if batch_idx > max_runs:\n",
    "#             break\n",
    "        \n",
    "        ### UPDATE MODEL PARAMETERS\n",
    "        optimizer.step()\n",
    "        \n",
    "        ### LOGGING\n",
    "        if not batch_idx % 200:\n",
    "            print ('Epoch: %03d/%03d | Batch %03d/%03d | Cost: %.4f' \n",
    "                   %(epoch+1, NUM_EPOCHS, batch_idx, \n",
    "                     len(train_dataset)//BATCH_SIZE, cost))\n",
    "    \n",
    "    \n",
    "#     if runs > max_runs:\n",
    "#         break\n",
    "    \n",
    "    with torch.set_grad_enabled(False):\n",
    "        avg_cost = avg_cost/len(train_dataset)\n",
    "        epoch_costs.append(avg_cost)\n",
    "        print('Epoch: %03d/%03d training accuracy: %.2f%%' % (\n",
    "              epoch+1, NUM_EPOCHS, \n",
    "              compute_accuracy(model, train_loader)))\n",
    "        print('Time elapsed: %.2f min' % ((time.time() - start_time)/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-6.1182e+10,  6.2020e+10, -6.1386e+10], grad_fn=<SelectBackward>)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([0., 1., 0.], grad_fn=<SelectBackward>)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor(0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor(5.9103e+11, grad_fn=<NllLossBackward>)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "num = 0\n",
    "for batch_idx, (features, targets) in enumerate(test_loader):\n",
    "    logits, probas = model(features)\n",
    "    display(logits[num])\n",
    "    display(probas[num])\n",
    "    display(targets[0])\n",
    "    display(F.cross_entropy(logits, targets))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.randn(3, 5, requires_grad=True)\n",
    "target = torch.empty(3, dtype=torch.long).random_(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.3922,  0.9763, -0.4188, -0.5152,  1.2310], grad_fn=<SelectBackward>)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(input[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(target[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.0278, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.cross_entropy(input, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
